#!/usr/bin/env bash

# Define the URL and output file
url="https://www.canada.ca/fr/environnement-changement-climatique/services/manuels-documents-conditions-meteorologiques/manab-abreviations-de-mots-4e-edition/recherche.html"
outputFile="config/manab_fr.csv"

# Use wget with custom headers
content=$(wget --header="Accept-Encoding: gzip, deflate, br" \
     --header="Accept-Language: en-US,en;q=0.5" \
     --header="DNT: 1" \
     --header="Connection: keep-alive" \
     --header="Upgrade-Insecure-Requests: 1" \
     --header="Sec-Fetch-Dest: document" \
     --header="Sec-Fetch-Mode: navigate" \
     --header="Sec-Fetch-Site: none" \
     --header="Sec-Fetch-User: ?1" \
     --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8" \
     --header="User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0" \
     -O - "$url" | gunzip -c)

echo "$content" | pup 'main.container table tbody tr json{}' | \
jq -r '.[] | [if (.children[0].text | type) == "string" then .children[0].text | gsub("\n";" ") | gsub("\\s+";" ") else .children[0].text end, (.children[1].text | gsub("\n";" ") | gsub("\\s+";" "))] | @csv' > "$outputFile"

echo "Data extracted to $outputFile"

# At this point, you need to parse 'page.html' and extract the needed data
# This is highly dependent on the page's structure and requires further commands

echo "Data extraction is a manual process based on page.html structure."
